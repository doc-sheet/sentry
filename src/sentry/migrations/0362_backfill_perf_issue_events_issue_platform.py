# Generated by Django 2.2.28 on 2023-02-14 19:42
from datetime import datetime
from typing import Any, Mapping, Sequence, Tuple

from django.db import migrations

from sentry.event_manager import GroupInfo
from sentry.eventstore.models import Event
from sentry.issues.grouptype import (
    PerformanceConsecutiveDBQueriesGroupType,
    PerformanceFileIOMainThreadGroupType,
    PerformanceMNPlusOneDBQueriesGroupType,
    PerformanceNPlusOneAPICallsGroupType,
    PerformanceNPlusOneGroupType,
    PerformanceRenderBlockingAssetSpanGroupType,
    PerformanceSlowDBQueryGroupType,
    PerformanceUncompressedAssetsGroupType,
)
from sentry.issues.ingest import process_occurrence_data
from sentry.issues.issue_occurrence import IssueOccurrence, IssueOccurrenceData
from sentry.new_migrations.migrations import CheckedMigration
from sentry.snuba.dataset import Dataset, EntityKey


def backfill_eventstream(apps, schema_editor):
    """
    Backfills Performance-issue events from the transaction table to the IssuePlatform dataset(search_issues).
    1. Read all transactions that back a performance-issue(s) with the following criteria:
        a. non-null group_ids column
        b.
    2. Prepare and format each transaction to be submitted through the eventstream.
    3. Send it through the eventstream.
    """

    # TODO: need to make sure event_type is correct in eventstream.send()
    from sentry.issues.ingest import send_issue_occurrence_to_eventstream

    Group = apps.get_model("sentry", "Group")

    project_perf_issues = Group.objects.filter(
        type__in=(
            PerformanceSlowDBQueryGroupType.type_id,
            PerformanceRenderBlockingAssetSpanGroupType.type_id,
            PerformanceNPlusOneGroupType.type_id,
            PerformanceConsecutiveDBQueriesGroupType.type_id,
            PerformanceFileIOMainThreadGroupType.type_id,
            PerformanceNPlusOneAPICallsGroupType.type_id,
            PerformanceMNPlusOneDBQueriesGroupType.type_id,
            PerformanceUncompressedAssetsGroupType.type_id,
        )
    ).values_list("id", "project_id")

    # retrieve rows
    rows = _query_performance_issue_events(
        project_ids=[tupe[1] for tupe in project_perf_issues],
        start=datetime(2008, 5, 8),
        end=datetime.now(),
    )

    for row in rows:
        try:
            # don't need to store in node_store since it should be there already, maybe check and write if doesn't exists
            # create issue occurrence
            # save issue occurrence
            event, occurrence_data, group_id = _map_performance_issue_row_to_params(row)

            group: Group = Group.objects.get(id=group_id)

            occurrence, group_info = _save_issue_occurrence(occurrence_data, event, group)

            send_issue_occurrence_to_eventstream(event, occurrence, group_info)
        except Exception:
            print("Failed to process row")  # noqa: S002


def _map_performance_issue_row_to_params(
    row: Mapping[str, Any]
) -> Tuple[Event, IssueOccurrenceData, int]:
    # we need to do the inverse of TransactionProcessor (Event -> EventStreamPayload -> Row):
    # Row -> Event
    # some columns in the rows have been extracted out from the Event interfaces to a top-level row and stripped out
    # we might need to put them back in
    data = {
        "platform": "",
        "message": "",
        "tags": [],
        "breadcrumbs": {},
        "contexts": {},
        "extra": {},
        "logger": "",
        "metadata": {},
        "request": {},
        "spans": [],
        "transaction": "",
        "type": "",
        "user": {},
        "measurements": {},
        "breakdowns": {},
    }
    project_id = row["project_id"]
    event_id = row["event_id"]

    event: Event = Event(project_id=project_id, event_id=event_id, data=data)
    occurrence_data: IssueOccurrenceData = IssueOccurrenceData(
        id="",  # TODO: need to generate this
        project_id=project_id,
        event_id=event_id,
        fingerprint=_retrieve_fingerprints(event_id, project_id),
        issue_title="",
        subtitle="",
        resource_id=None,
        evidence_data="",
        evidence_display="",
        type="",
        detection_time="",
        level="",
    )

    return event, occurrence_data, int(row["group_id"])


def _retrieve_fingerprints(event_id: str, project_id: int) -> Sequence[str]:
    from sentry import eventstore

    event = eventstore.get_event_by_id(project_id, event_id)
    if event is None:
        raise ValueError(f"no event found in eventstore({project_id}, {event_id})")

    # We always fetch the stored hashes here.  The reason for this is
    # that we want to show in the UI if the forced grouping algorithm
    # produced hashes that would normally also appear in the event.
    hashes = event.get_hashes()

    # Transactions events are grouped using performance detection. They
    # are not subject to grouping configs, and the only relevant
    # grouping variant is `PerformanceProblemVariant`.

    from sentry.utils.performance_issues.performance_detection import EventPerformanceProblem

    problems = EventPerformanceProblem.fetch_multi([(event, h) for h in hashes.hashes])
    # TODO: not sure how this can work
    # a transaction can have N number of issues created, resulting in N groups created
    # we might have to do a GroupHash lookup here to find the fingerprint since we cant map them
    # neatly from just the event
    return [problem.problem.fingerprint for problem in problems or ()]


def _query_performance_issue_events(
    project_ids: Sequence[int], start: datetime, end: datetime
) -> Sequence[Mapping[str, Any]]:
    from snuba_sdk import Column, Condition, Entity, Function, Op, Query, Request

    snuba_request = Request(
        dataset=Dataset.Transactions.value,
        app_id="migration",
        query=Query(
            match=Entity(EntityKey.Transactions.value),
            select=[
                Function("arrayJoin", parameters=[Column("group_ids")], alias="group_id"),
                Column("project_id"),
                Column("event_id"),
                # Column("trace_id"),
                # Column("span_id"),
                # Column("transaction_name"),
                # Column("transaction_hash"),
                # Column("transaction_op"),
                # Column("transaction_status"),
                # Column("start_ts"),
                # Column("start_ms"),
                # Column("finish_ts"),
                # Column("finish_ms"),
                # Column("duration"),
                # Column("platform"),
                # Column("environment"),
                # Column("release"),
                # Column("dist"),
                # Column("ip_address_v4"),
                # Column("ip_address_v6"),
                # Column("user"),
                # Column("user_id"),
                # Column("user_name"),
                # Column("user_email"),
                # Column("sdk_name"),
                # Column("sdk_version"),
                # Column("http_method"),
                # Column("http_referer"),
                # Column("tags"),
                # Column("contexts"),
                # Column("measurements"),
                # Column("span_op_breakdowns"),
                # Column("spans"),
                # Column("type"),
                # Column("message"),
                # Column("title"),
                # Column("transaction_source"),
                # Column("timestamp"),
                # Column("app_start_type"),
                # Column("profile_id"),
            ],
            where=[
                Condition(Column("group_ids"), Op.IS_NOT_NULL),
                Condition(Column("project_id"), Op.IN, project_ids),
                Condition(Column("finish_ts"), Op.GTE, start),
                Condition(Column("finish_ts"), Op.LT, end),
            ],
        ),
    )
    from sentry.utils.snuba import raw_snql_query

    result_snql = raw_snql_query(
        snuba_request,
        referrer="0359_duplicate_perf_issue_events_issue_platform._query_performance_issue_events",
        use_cache=False,
    )

    return result_snql["data"]


def _save_issue_occurrence(
    occurrence_data: IssueOccurrenceData, event: Event, group
) -> Tuple[IssueOccurrence, GroupInfo]:
    process_occurrence_data(occurrence_data)
    # Convert occurrence data to `IssueOccurrence`
    occurrence = IssueOccurrence.from_dict(occurrence_data)
    if occurrence.event_id != event.event_id:
        raise ValueError("IssueOccurrence must have the same event_id as the passed Event")
    # Note: For now we trust the project id passed along with the event. Later on we should make
    # sure that this is somehow validated.
    occurrence.save()

    # don't need to create releases or environments since they should be created already

    # synthesize a 'fake' group_info based off of existing data in postgres
    group_info: GroupInfo = GroupInfo(group=group, is_new=False, is_regression=False)

    return occurrence, group_info


class Migration(CheckedMigration):
    # This flag is used to mark that a migration shouldn't be automatically run in production. For
    # the most part, this should only be used for operations where it's safe to run the migration
    # after your code has deployed. So this should not be used for most operations that alter the
    # schema of a table.
    # Here are some things that make sense to mark as dangerous:
    # - Large data migrations. Typically we want these to be run manually by ops so that they can
    #   be monitored and not block the deploy for a long period of time while they run.
    # - Adding indexes to large tables. Since this can take a long time, we'd generally prefer to
    #   have ops run this and not block the deploy. Note that while adding an index is a schema
    #   change, it's completely safe to run the operation after the code has deployed.
    is_dangerous = True

    dependencies = [
        ("sentry", "0361_monitor_environment"),
    ]

    operations = [
        migrations.RunPython(
            backfill_eventstream,
            reverse_code=migrations.RunPython.noop,
            hints={"tables": ["sentry_grouphash", "sentry_groupedmessage"]},
        )
    ]
